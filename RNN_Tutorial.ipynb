{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hihello를 말하는 모델 만들기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 준비하기\n",
    "\n",
    "모델이 자연어를 학습하도록 하려면 자연어를 벡터로 만들어줘야합니다.  \n",
    "벡터로 만드는 방법은 여러가지가 있으나, 여기서는 가장 간단한 one-hot vector로 만들어 보도록하겠습니다.  \n",
    "\n",
    "<img src='./img/2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict = ['h', 'i', 'e' , 'l' , 'o']\n",
    "\n",
    "x_data = [0, 1, 0, 2, 3, 3] #hihell\n",
    "\n",
    "x_one_hot = [[[1,0,0,0,0],  #h\n",
    "              [0,1,0,0,0],  #i\n",
    "              [1,0,0,0,0],  #h\n",
    "              [0,0,1,0,0],  #e\n",
    "              [0,0,0,1,0],  #l\n",
    "              [0,0,0,1,0]]] #l\n",
    "\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4] #ihello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 변환\n",
    "inputs = torch.Tensor(x_one_hot)\n",
    "labels = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 5 # vector의 size : one_hot vector의 size는 5\n",
    "num_classes = 5\n",
    "hidden_size = 5\n",
    "batch_size = 1\n",
    "sequence_length = 6\n",
    "num_layers = 1 # vanilla RNN이므로 layer는 1개\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model  준비하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/3.png' width='400'> <img src='./img/4.png' width = '400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = x.view(batch_size, sequence_length, input_size) # batch_first이므로 batch_size가 먼저 오게끔 reshape\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = output.view(-1, num_classes)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model  만들기\n",
    "\n",
    "* Loss Function : CrossEntropyLoss\n",
    "* Optimization : Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Bulid the Model---------\n",
      "epoch : 1, loss : 1.787, 예측 결과 : heeeee\n",
      "epoch : 2, loss : 1.688, 예측 결과 : heeeee\n",
      "epoch : 3, loss : 1.599, 예측 결과 : heelee\n",
      "epoch : 4, loss : 1.522, 예측 결과 : eeelee\n",
      "epoch : 5, loss : 1.455, 예측 결과 : elelel\n",
      "epoch : 6, loss : 1.397, 예측 결과 : elelll\n",
      "epoch : 7, loss : 1.348, 예측 결과 : llelll\n",
      "epoch : 8, loss : 1.305, 예측 결과 : llelll\n",
      "epoch : 9, loss : 1.265, 예측 결과 : ilelll\n",
      "epoch : 10, loss : 1.225, 예측 결과 : ilelll\n",
      "epoch : 11, loss : 1.183, 예측 결과 : ilello\n",
      "epoch : 12, loss : 1.139, 예측 결과 : ilello\n",
      "epoch : 13, loss : 1.096, 예측 결과 : ilello\n",
      "epoch : 14, loss : 1.056, 예측 결과 : ilello\n",
      "epoch : 15, loss : 1.020, 예측 결과 : ilello\n",
      "epoch : 16, loss : 0.987, 예측 결과 : ihello\n",
      "epoch : 17, loss : 0.957, 예측 결과 : ihello\n",
      "epoch : 18, loss : 0.928, 예측 결과 : ihello\n",
      "epoch : 19, loss : 0.900, 예측 결과 : ihello\n",
      "epoch : 20, loss : 0.873, 예측 결과 : ihello\n",
      "epoch : 21, loss : 0.847, 예측 결과 : ihello\n",
      "epoch : 22, loss : 0.824, 예측 결과 : ihello\n",
      "epoch : 23, loss : 0.803, 예측 결과 : ihello\n",
      "epoch : 24, loss : 0.785, 예측 결과 : ihello\n",
      "epoch : 25, loss : 0.769, 예측 결과 : ihello\n",
      "epoch : 26, loss : 0.756, 예측 결과 : ihello\n",
      "epoch : 27, loss : 0.744, 예측 결과 : ihello\n",
      "epoch : 28, loss : 0.733, 예측 결과 : ihello\n",
      "epoch : 29, loss : 0.723, 예측 결과 : ihello\n",
      "epoch : 30, loss : 0.715, 예측 결과 : ihello\n",
      "epoch : 31, loss : 0.707, 예측 결과 : ihello\n",
      "epoch : 32, loss : 0.700, 예측 결과 : ihello\n",
      "epoch : 33, loss : 0.694, 예측 결과 : ihello\n",
      "epoch : 34, loss : 0.687, 예측 결과 : ihello\n",
      "epoch : 35, loss : 0.680, 예측 결과 : ihello\n",
      "epoch : 36, loss : 0.674, 예측 결과 : ihello\n",
      "epoch : 37, loss : 0.668, 예측 결과 : ihello\n",
      "epoch : 38, loss : 0.663, 예측 결과 : ihello\n",
      "epoch : 39, loss : 0.658, 예측 결과 : ihello\n",
      "epoch : 40, loss : 0.653, 예측 결과 : ihello\n",
      "epoch : 41, loss : 0.648, 예측 결과 : ihello\n",
      "epoch : 42, loss : 0.643, 예측 결과 : ihello\n",
      "epoch : 43, loss : 0.638, 예측 결과 : ihello\n",
      "epoch : 44, loss : 0.634, 예측 결과 : ihello\n",
      "epoch : 45, loss : 0.629, 예측 결과 : ihello\n",
      "epoch : 46, loss : 0.624, 예측 결과 : ihello\n",
      "epoch : 47, loss : 0.619, 예측 결과 : ihello\n",
      "epoch : 48, loss : 0.615, 예측 결과 : ihello\n",
      "epoch : 49, loss : 0.611, 예측 결과 : ihello\n",
      "epoch : 50, loss : 0.606, 예측 결과 : ihello\n",
      "epoch : 51, loss : 0.602, 예측 결과 : ihello\n",
      "epoch : 52, loss : 0.599, 예측 결과 : ihello\n",
      "epoch : 53, loss : 0.595, 예측 결과 : ihello\n",
      "epoch : 54, loss : 0.591, 예측 결과 : ihello\n",
      "epoch : 55, loss : 0.588, 예측 결과 : ihello\n",
      "epoch : 56, loss : 0.584, 예측 결과 : ihello\n",
      "epoch : 57, loss : 0.581, 예측 결과 : ihello\n",
      "epoch : 58, loss : 0.577, 예측 결과 : ihello\n",
      "epoch : 59, loss : 0.574, 예측 결과 : ihello\n",
      "epoch : 60, loss : 0.570, 예측 결과 : ihello\n",
      "epoch : 61, loss : 0.567, 예측 결과 : ihello\n",
      "epoch : 62, loss : 0.564, 예측 결과 : ihello\n",
      "epoch : 63, loss : 0.561, 예측 결과 : ihello\n",
      "epoch : 64, loss : 0.559, 예측 결과 : ihello\n",
      "epoch : 65, loss : 0.556, 예측 결과 : ihello\n",
      "epoch : 66, loss : 0.553, 예측 결과 : ihello\n",
      "epoch : 67, loss : 0.550, 예측 결과 : ihello\n",
      "epoch : 68, loss : 0.548, 예측 결과 : ihello\n",
      "epoch : 69, loss : 0.545, 예측 결과 : ihello\n",
      "epoch : 70, loss : 0.543, 예측 결과 : ihello\n",
      "epoch : 71, loss : 0.541, 예측 결과 : ihello\n",
      "epoch : 72, loss : 0.538, 예측 결과 : ihello\n",
      "epoch : 73, loss : 0.536, 예측 결과 : ihello\n",
      "epoch : 74, loss : 0.534, 예측 결과 : ihello\n",
      "epoch : 75, loss : 0.532, 예측 결과 : ihello\n",
      "epoch : 76, loss : 0.530, 예측 결과 : ihello\n",
      "epoch : 77, loss : 0.528, 예측 결과 : ihello\n",
      "epoch : 78, loss : 0.526, 예측 결과 : ihello\n",
      "epoch : 79, loss : 0.524, 예측 결과 : ihello\n",
      "epoch : 80, loss : 0.522, 예측 결과 : ihello\n",
      "epoch : 81, loss : 0.521, 예측 결과 : ihello\n",
      "epoch : 82, loss : 0.519, 예측 결과 : ihello\n",
      "epoch : 83, loss : 0.517, 예측 결과 : ihello\n",
      "epoch : 84, loss : 0.516, 예측 결과 : ihello\n",
      "epoch : 85, loss : 0.514, 예측 결과 : ihello\n",
      "epoch : 86, loss : 0.513, 예측 결과 : ihello\n",
      "epoch : 87, loss : 0.511, 예측 결과 : ihello\n",
      "epoch : 88, loss : 0.510, 예측 결과 : ihello\n",
      "epoch : 89, loss : 0.509, 예측 결과 : ihello\n",
      "epoch : 90, loss : 0.507, 예측 결과 : ihello\n",
      "epoch : 91, loss : 0.506, 예측 결과 : ihello\n",
      "epoch : 92, loss : 0.505, 예측 결과 : ihello\n",
      "epoch : 93, loss : 0.504, 예측 결과 : ihello\n",
      "epoch : 94, loss : 0.503, 예측 결과 : ihello\n",
      "epoch : 95, loss : 0.502, 예측 결과 : ihello\n",
      "epoch : 96, loss : 0.501, 예측 결과 : ihello\n",
      "epoch : 97, loss : 0.500, 예측 결과 : ihello\n",
      "epoch : 98, loss : 0.499, 예측 결과 : ihello\n",
      "epoch : 99, loss : 0.498, 예측 결과 : ihello\n",
      "epoch : 100, loss : 0.497, 예측 결과 : ihello\n"
     ]
    }
   ],
   "source": [
    "rnn = Model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = 0.04)\n",
    "\n",
    "print(\"--------Bulid the Model---------\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = rnn.init_hidden()\n",
    "    \n",
    "    outputs = rnn(inputs, hidden)\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    _, idx = outputs.max(1)\n",
    "    \n",
    "    results_str = [char_dict[c] for c in idx.squeeze()]\n",
    "    \n",
    "    print(\"epoch : %d, loss : %1.3f, 예측 결과 : %s\" %(epoch+1, loss.data.item(), ''.join(results_str)))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
